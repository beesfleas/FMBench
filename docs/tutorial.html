<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quickstart Guide - FMBench</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="sidebar">
        <h2>FMBench</h2>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="slides.html">Slides</a></li>
                <li><a href="report.html">Report</a></li>
                <li><a href="data_appendix.html">Data Appendix</a></li>
                <li><a href="supplementary.html">Supplementary</a></li>
                <li><a href="tutorial.html" class="active">Quickstart Guide</a></li>
                <li><a href="references.html">References</a></li>
            </ul>
        </nav>
    </div>

    <div class="main-content">
        <h1>Quickstart Guide</h1>

        <section class="section">
            <h2>Installation</h2>

            <h3>Prerequisites</h3>
            <ul>
                <li>Python 3.8+</li>
                <li>pip</li>
            </ul>

            <h3>Setup</h3>
            <pre><code># Clone the repository
git clone https://github.com/beesfleas/FMBench
cd FMBench/software

# Install dependencies
pip install -r requirements.txt

# (Optional) For quantization support
pip install bitsandbytes</code></pre>
        </section>

        <hr>

        <section class="section">
            <h2>Model Classes</h2>
            <p>FMBench supports three main classes of models:</p>
            <ul>
                <li><strong>LLM (Large Language Model)</strong>: For text generation and NLP tasks.</li>
                <li><strong>VLM (Vision Language Model)</strong>: For tasks involving both images and text.</li>
                <li><strong>Time Series</strong>: For forecasting and time-series analysis tasks.</li>
            </ul>

            <h2>Available Scenarios</h2>
            <p>Scenarios define the tasks and datasets used for benchmarking. Here are the available scenarios:</p>

            <h3>Language & Knowledge (LLM)</h3>
            <ul>
                <li><code>sentiment</code>: Sentiment analysis on IMDB movie reviews.</li>
                <li><code>summarization</code>: Abstractive summarization using CNN/DailyMail.</li>
                <li><code>ner</code>: Named Entity Recognition on WikiANN.</li>
                <li><code>classification</code>: Text classification on AG News.</li>
                <li><code>translation</code>: Machine translation (German to English) on WMT16.</li>
                <li><code>arc_challenge</code> / <code>arc_easy</code>: AI2 Reasoning Challenge.</li>
                <li><code>mmlu</code>: Massive Multitask Language Understanding benchmark.</li>
                <li><code>helm</code>: Holistic Evaluation of Language Models.</li>
                <li><code>perplexity_c4</code> / <code>perplexity_wikitext2</code>: Perplexity evaluation.</li>
                <li><code>long_context</code>: Long context window evaluation.</li>
            </ul>

            <h3>Vision & Multimodal (VLM)</h3>
            <ul>
                <li><code>countbenchqa</code>: Visual counting question answering.</li>
                <li><code>docvqa</code>: Document OCR question answering.</li>
                <li><code>vqa</code>: Visual Question Answering v2.</li>
                <li><code>gtsrb</code>: German Traffic Sign Recognition.</li>
                <li><code>hagrid</code>: Hand Gesture Recognition.</li>
                <li><code>simple_vlm</code>: Basic VLM test scenario.</li>
            </ul>

            <h3>Time Series</h3>
            <ul>
                <li><code>ett</code>: Electricity Transformer Temperature forecasting.</li>
                <li><code>gifteval</code>: Zero-shot time series forecasting.</li>
                <li><code>m3_monthly</code>: M3 competition monthly forecasting.</li>
                <li><code>simple_timeseries</code>: Basic time series test scenario.</li>
            </ul>

            <h2>Running a Benchmark</h2>
            <p>To run a benchmark, use the <code>run.py</code> script. You can configure the run using flags.</p>

            <pre><code>python run.py</code></pre>

            <h3>Common Flags</h3>
            <ul>
                <li><code>model=&lt;model_name&gt;</code>: (string) Select a specific model configuration.</li>
                <li><code>scenario=&lt;scenario_name&gt;</code>: (string) Name of the scenario to run.</li>
                <li><code>num_samples=&lt;N&gt;</code>: (int) Number of samples to run from the dataset.</li>
                <li><code>device=&lt;device_type&gt;</code>: (string) Device to use (auto, cpu, gpu, mac, jetson, pi).
                </li>
                <li><code>model.loader_type=&lt;loader&gt;</code>: (string) Specific loader to use (e.g.,
                    <code>vllm</code>, <code>huggingface</code>).
                </li>
                <li><code>model.quantization=&lt;bits&gt;</code>: (int) Enable quantization (4 or 8 bit).</li>
                <li><code>model.max_tokens=&lt;N&gt;</code>: (int) Maximum tokens for generation.</li>
                <li><code>model.device_preference=&lt;device&gt;</code>: (string) Preferred device for model (auto, cpu,
                    cuda, mps).</li>
                <li><code>log_level=&lt;level&gt;</code>: (string) Logging level (DEBUG, INFO, WARNING, ERROR).</li>
                <li><code>save_logs=&lt;bool&gt;</code>: (bool) Save logs to file.</li>
                <li><code>sampling_interval=&lt;seconds&gt;</code>: (float) Profiling sampling interval in seconds.</li>
                <li><code>scenario.slo_threshold=&lt;N&gt;</code>: (int) SLO violation threshold for CountBenchQA (e.g.,
                    if |predicted - target| > N, it's a violation).</li>
            </ul>

            <h3>Example Commands</h3>
            <pre><code># Run sentiment analysis with a small model
python run.py model=distilgpt2 scenario=sentiment num_samples=100

# Run VLM counting task with SLO tracking
python run.py model=smolvlm scenario=countbenchqa scenario.slo_threshold=2

# Run with quantization on a large model
python run.py model=qwen2.5 scenario=summarization model.quantization=4

# Run time series forecasting
python run.py model=timegpt1 scenario=ett num_samples=50</code></pre>
        </section>

        <hr>

        <section class="section">
            <h2>Benchmark Suite</h2>
            <p>For comprehensive testing, FMBench allows you to run extensive suites across multiple model and scenario
                combinations using the <code>benchmark_suite.py</code> script.</p>

            <h3>Running a Suite</h3>
            <pre><code># Preview what will run (dry run)
python benchmark_suite.py --summary-only

# Run the suite (with confirmation prompt)
python benchmark_suite.py

# Run immediately (skip confirmation)
python benchmark_suite.py -y</code></pre>

            <h3>Device Level Filtering</h3>
            <p>You can filter the test matrix based on the capability of your target device:</p>
            <ul>
                <li><code>--device-level SoC</code>: Restricts models to those suitable for edge devices (≤1B params,
                    e.g., Raspberry Pi).</li>
                <li><code>--device-level Mobile</code>: Targets mobile-class hardware (≤3B params).</li>
                <li><code>--device-level Server</code>: Runs all configured models (No limit). Default behavior.</li>
            </ul>
            <pre><code>python benchmark_suite.py --device-level SoC</code></pre>

            <h3>Outputs & Visualization</h3>
            <p>After a suite run completes, FMBench automatically generates comparative visualizations in the
                <code>suite_logs/&lt;log_name&gt;_graphs/</code> directory.</p>

            <h4>Summary Plots</h4>
            <ul>
                <li><strong>Latency vs Accuracy</strong>: <code>summary_latency_vs_accuracy.png</code></li>
                <li><strong>Latency vs Energy</strong>: <code>summary_latency_vs_energy.png</code></li>
                <li><strong>Accuracy vs Energy</strong>: <code>summary_accuracy_vs_energy.png</code></li>
            </ul>

            <h4>Per-Scenario Plots</h4>
            <ul>
                <li>Detailed scatter plots for specific scenarios (e.g.,
                    <code>summarization_latency_vs_accuracy.png</code>).</li>
                <li><strong>sMAPE Bar Charts</strong>: For time-series tasks
                    (<code>&lt;scenario&gt;_smape_bar.png</code>).</li>
            </ul>

            <h4>LaTeX Report</h4>
            <p>A comprehensive <code>results.tex</code> file is generated, containing LaTeX tables for specific hardware
                metrics, model comparisons, and scenario breakdowns, ready for inclusion in academic papers or technical
                reports.</p>

            <h3>Manual Graph Generation</h3>
            <p>If you need to regenerate plots or combine results from multiple distinct runs, use the
                <code>generate_graphs.py</code> utility:</p>
            <pre><code># Generate graphs from a single log
python software/generate_graphs.py suite_logs/suite_20240101.log

# Combine multiple logs into one set of graphs
python software/generate_graphs.py suite_logs/run1.log suite_logs/run2.log</code></pre>
        </section>

        <footer>
            <p>&copy; 2025 FMBench Project</p>
        </footer>
    </div>
</body>

</html>