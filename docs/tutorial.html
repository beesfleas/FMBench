<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quickstart Guide - FMBench</title>
    <link rel="stylesheet" href="style.css">
</head>

<body>
    <div class="sidebar">
        <h2>FMBench</h2>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="slides.html">Slides</a></li>
                <li><a href="report.html">Report</a></li>
                <li><a href="data_appendix.html">Data Appendix</a></li>
                <li><a href="supplementary.html">Supplementary</a></li>
                <li><a href="tutorial.html" class="active">Quickstart Guide</a></li>
                <li><a href="references.html">References</a></li>
            </ul>
        </nav>
    </div>

    <div class="main-content">
        <h1>Quickstart Guide</h1>

        <section class="section">
            <h2>Installation</h2>

            <h3>Prerequisites</h3>
            <ul>
                <li>Python 3.8+</li>
                <li>pip</li>
            </ul>

            <h3>Setup</h3>
            <pre><code># Clone the repository
git clone https://github.com/beesfleas/FMBench
cd FMBench/software

# Install dependencies
pip install -r requirements.txt

# (Optional) For quantization support
pip install bitsandbytes</code></pre>
        </section>

        <hr>

        <section class="section">
            <h2>Model Classes</h2>
            <p>FMBench supports three main classes of models:</p>
            <ul>
                <li><strong>LLM (Large Language Model)</strong>: For text generation and NLP tasks.</li>
                <li><strong>VLM (Vision Language Model)</strong>: For tasks involving both images and text.</li>
                <li><strong>Time Series</strong>: For forecasting and time-series analysis tasks.</li>
            </ul>

            <h2>Available Scenarios</h2>
            <p>Scenarios define the tasks and datasets used for benchmarking. Here are the available scenarios:</p>

            <h3>Language & Knowledge (LLM)</h3>
            <ul>
                <li><code>sentiment</code>: Sentiment analysis on IMDB movie reviews.</li>
                <li><code>summarization</code>: Abstractive summarization using CNN/DailyMail.</li>
                <li><code>ner</code>: Named Entity Recognition on WikiANN.</li>
                <li><code>classification</code>: Text classification on AG News.</li>
                <li><code>translation</code>: Machine translation (German to English) on WMT16.</li>
                <li><code>arc_challenge</code> / <code>arc_easy</code>: AI2 Reasoning Challenge.</li>
                <li><code>mmlu</code>: Massive Multitask Language Understanding benchmark.</li>
                <li><code>helm</code>: Holistic Evaluation of Language Models.</li>
                <li><code>perplexity_c4</code> / <code>perplexity_wikitext2</code>: Perplexity evaluation.</li>
                <li><code>long_context</code>: Long context window evaluation.</li>
            </ul>

            <h3>Vision & Multimodal (VLM)</h3>
            <ul>
                <li><code>countbenchqa</code>: Visual counting question answering.</li>
                <li><code>docvqa</code>: Document OCR question answering.</li>
                <li><code>vqa</code>: Visual Question Answering v2.</li>
                <li><code>gtsrb</code>: German Traffic Sign Recognition.</li>
                <li><code>hagrid</code>: Hand Gesture Recognition.</li>
                <li><code>simple_vlm</code>: Basic VLM test scenario.</li>
            </ul>

            <h3>Time Series</h3>
            <ul>
                <li><code>ett</code>: Electricity Transformer Temperature forecasting.</li>
                <li><code>gifteval</code>: Zero-shot time series forecasting.</li>
                <li><code>m3_monthly</code>: M3 competition monthly forecasting.</li>
                <li><code>simple_timeseries</code>: Basic time series test scenario.</li>
            </ul>

            <h2>Running a Benchmark</h2>
            <p>To run a benchmark, use the <code>run.py</code> script. You can configure the run using flags.</p>

            <pre><code>python run.py</code></pre>

            <h3>Common Flags</h3>
            <ul>
                <li><code>model=&lt;model_name&gt;</code>: (string) Select a specific model configuration.</li>
                <li><code>scenario=&lt;scenario_name&gt;</code>: (string) Name of the scenario to run.</li>
                <li><code>num_samples=&lt;N&gt;</code>: (int) Number of samples to run from the dataset.</li>
                <li><code>device=&lt;device_type&gt;</code>: (string) Device to use (auto, cpu, gpu, mac, jetson, pi).
                </li>
                <li><code>model.loader_type=&lt;loader&gt;</code>: (string) Specific loader to use (e.g.,
                    <code>vllm</code>, <code>huggingface</code>).</li>
                <li><code>model.quantization=&lt;bits&gt;</code>: (int) Enable quantization (4 or 8 bit).</li>
                <li><code>model.max_tokens=&lt;N&gt;</code>: (int) Maximum tokens for generation.</li>
                <li><code>model.device_preference=&lt;device&gt;</code>: (string) Preferred device for model (auto, cpu,
                    cuda, mps).</li>
                <li><code>log_level=&lt;level&gt;</code>: (string) Logging level (DEBUG, INFO, WARNING, ERROR).</li>
                <li><code>save_logs=&lt;bool&gt;</code>: (bool) Save logs to file.</li>
                <li><code>sampling_interval=&lt;seconds&gt;</code>: (float) Profiling sampling interval in seconds.</li>
                <li><code>scenario.slo_threshold=&lt;N&gt;</code>: (int) SLO violation threshold for CountBenchQA (e.g.,
                    if |predicted - target| > N, it's a violation).</li>
            </ul>

            <h3>Example Commands</h3>
            <pre><code># Run sentiment analysis with a small model
python run.py model=distilgpt2 scenario=sentiment num_samples=100

# Run VLM counting task with SLO tracking
python run.py model=smolvlm scenario=countbenchqa scenario.slo_threshold=2

# Run with quantization on a large model
python run.py model=qwen2.5 scenario=summarization model.quantization=4

# Run time series forecasting
python run.py model=timegpt1 scenario=ett num_samples=50</code></pre>
        </section>

        <footer>
            <p>&copy; 2025 FMBench Project</p>
        </footer>
    </div>
</body>

</html>