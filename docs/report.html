<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <!DOCTYPE html>
    <html lang="en">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Report - HoliBench</title>
        <link rel="stylesheet" href="style.css">
    </head>

<body>
    <div class="sidebar">
        <h2>HoliBench</h2>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="slides.html">Slides</a></li>
                <li><a href="report.html" class="active">Report</a></li>
                <li><a href="data_appendix.html">Data Appendix</a></li>
                <li><a href="supplementary.html">Supplementary</a></li>
                <li><a href="tutorial.html">Tutorial</a></li>
                <li><a href="references.html">References</a></li>
            </ul>
        </nav>
    </div>

    <div class="main-content">
        <h1>Project Report</h1>

        <div class="toc">
            <h3>Table of Contents</h3>
            <ol>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#related-work">Related Work</a></li>
                <li><a href="#technical-approach">Technical Approach</a></li>
                <li><a href="#evaluation">Evaluation and Results</a></li>
                <li><a href="#discussions">Discussions and Conclusions</a></li>
                <li><a href="#references">References</a></li>
            </ol>
        </div>

        <hr>

        <section id="introduction" class="section">
            <h2>1. Introduction: Foundation Model Performance Benchmarking</h2>
            <p>HoliBench is a flexible benchmarking framework designed to evaluate foundation models across diverse
                hardware platforms.
                To understand our approach, we must first look at how foundation models are typically benchmarked today.
                Popular models often compete intensely to see who can achieve better scores on specific capability
                metrics.
            </p>

            <p>These capability metrics vary by model type:</p>
            <ul>
                <li><strong>Large Language Models (LLMs)</strong>: Evaluated on <em>Perplexity</em> (how well they
                    predict the next token), <em>Reasoning</em> (logic and problem solving), and tasks like
                    <em>Summarization</em>.
                </li>
                <li><strong>Visual Language Models (VLMs)</strong>: Tested via <em>Visual Reasoning Tests</em>, <em>OCR
                        Tests</em> (reading text in images), and <em>Counting Tests</em>.</li>
                <li><strong>Time Series Models</strong>: Assessed using statistical error metrics like <em>SMAPE</em>
                    (Symmetric Mean Absolute Percentage Error) and <em>MSLE</em> (Mean Squared Logarithmic Error).</li>
            </ul>

            <h3>1.1 Performance Benchmarking Landscapes</h3>
            <p>A variety of well-established benchmarks exist solely to determine how "good" a model is compared to
                others in terms of these performance capabilities. Prominent examples include:</p>
            <ul>
                <li><strong><a href="references.html#ref-helm">HELM (Holistic Evaluation of Language
                            Models)</a></strong> from Liang et al. (Stanford CRFM), 2022: Evaluates models across a wide
                    breadth of scenarios to ensure holistic coverage.</li>
                <li><strong><a href="references.html#ref-bigbench">Big-Bench (Beyond the Imitation Game
                            Benchmark)</a></strong> from Srivastava et al. (Google), 2022: A diverse, collaborative
                    benchmark for measuring capabilities.</li>
                <li><strong><a href="references.html#ref-lmsys">LMSYS Chatbot Arena</a></strong> from Chiang et al. (UC
                    Berkeley), 2024: Uses an Elo rating system based on human pairwise comparisons.</li>
                <li><strong><a href="references.html#ref-hf-leaderboard">Hugging Face Open LLM leaderboard</a></strong>
                    from Beeching et al. (Hugging Face), 2024: Automates evaluation on standard datasets to rank open
                    models.</li>
            </ul>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/onePillar.png" alt="Performance Benchmarks Only"
                    style="height: 250px; width: auto;">
                <figcaption>Figure 1: Current Benchmarking Landscape focuses heavily on Model Capability (Performance).
                </figcaption>
            </figure>

            <h3>1.2 Hardware Level Metrics</h3>
            <p>While model capability is crucial, it is not the only factor that matters. <strong>Hardware level
                    metrics</strong> are equally critical, especially when deploying models on edge devices, IoT
                sensors, or resource-constrained environments. People are actively solving these efficiency issues and
                benchmarking them.</p>

            <p>Examples of this focus include:</p>
            <ul>
                <li><strong><a href="references.html#ref-neuricam">NeuriCam</a></strong>: A system for key-frame video
                    super-resolution on power-constrained IoT cameras.</li>
                <li><strong><a href="references.html#ref-flashattention">FlashAttention</a></strong>: Introduced by Tri
                    Dao et al., this innovation significantly improved latency by optimizing IO-awareness, benefiting
                    both datacenter and edge deployments.</li>
                <li>Research on <strong><a href="references.html#ref-micro-npu">Benchmarking Ultra-Low-Power
                            micro-NPUs</a></strong> demonstrates the need for specialized evaluation on tiny silicon.
                </li>
            </ul>

            <p>Typically, there are two key metrics for models running on IoTs and SoCs:</p>
            <ul>
                <li><strong>Energy</strong>: Small devices are power-constrained (battery life) and subject to carbon
                    emission concerns.</li>
                <li><strong>Latency</strong>: Real-time tasks require fast responses. Edge devices have less compute,
                    making them naturally slower and more sensitive to inefficiency.</li>
            </ul>

            <p>Several works have addressed these hardware-centric metrics. <strong><a
                        href="references.html#ref-pagoda">Pagoda</a></strong> utilizes a roofline model to analyze
                latency vs. energy and latency vs. performance limits. Other notable efficiency benchmarks include
                <strong><a href="references.html#ref-mltiny">MLPerf Tiny</a></strong> and <strong><a
                        href="references.html#ref-elib">ELIB</a></strong>.
            </p>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/twoPillar.png" alt="Energy and Latency Benchmarks Only"
                    style="height: 250px; width: auto;">
                <figcaption>Figure 2: The Efficiency Benchmarking Landscape focuses on Energy and Latency.</figcaption>
            </figure>

            <h3>1.3 HoliBench</h3>

            <h4>1.3.1 Limitations</h4>
            <p><strong>Hardware Level Fragmentation:</strong> Developers currently have to use different, incompatible
                tools for every hardware platform to measure system efficiency.</p>
            <ul>
                <li><strong>NVIDIA GPUs</strong>: Require interfacing with NVML or spawning nvidia-smi subprocesses.
                </li>
                <li><strong>Apple Silicon</strong>: Require powermetrics (CLI tool).</li>
                <li><strong>Jetson Edge Devices</strong>: Require jetson-stats library to read specialized sensors on
                    Orin/Xavier.</li>
                <li><strong>CPUs</strong>: Require psutil for OS kernel counters.</li>
                <li><strong>Raspberry Pi</strong>: Uses vcgencmd and pmic_read_adc to query onboard Power Management
                    ICs.</li>
            </ul>

            <p><strong>Benchmark Gaps:</strong> Current benchmarking ecosystems suffer from critical limitations:</p>
            <ul>
                <li>Most benchmarks focus on server-grade hardware or pure inference speed.</li>
                <li>There is a lack of Edge-focused benchmarks that treat Energy and Latency as first-class citizens
                    alongside service level goals.</li>
                <li>Small devices are constrained by power and thermal limits (carbon emissions/heat), making these
                    metrics critical for IoT/SoCs.</li>
                <li>There is a need for a library allowing anyone to benchmark models easily across these diverse
                    constraints.</li>
            </ul>

            <h4>1.3.2 Novelty</h4>
            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/threePillar.png" alt="HoliBench Three Pillar Approach"
                    style="height: 250px; width: auto;">
                <figcaption>Figure 3: HoliBench bridges the gap by unifying Performance, Energy, and Latency.
                </figcaption>
            </figure>

            <p>HoliBench introduces several novel contributions to address these challenges:</p>
            <ul>
                <li><strong>Unified "Three Pillar" Approach</strong>: We unify three primary metrics — Latency, Energy,
                    and Performance — into one comprehensive report.</li>
                <li><strong>Modular Design</strong>: Allows for easy addition of new models, scenarios, and hardware
                    profilers.</li>
                <li><strong>Standardization and Control</strong>: Provides a consistent interface for benchmarking
                    across disparate hardware.</li>
                <li><strong>Ease of Use</strong>: YAML-based configuration makes running complex benchmarks simple.</li>
                <li><strong>Hardware Profiler Wrapper</strong>: Features a specialized context manager
                    (<code>__enter__</code>/<code>__exit__</code>) to ensure all profilers start and stop synchronously
                    with the model inference.</li>
            </ul>
        </section>

        <!-- Related Work section removed as it is now integrated into Introduction as subsections 1.1 and 1.2 -->

        <section id="technical-approach" class="section">
            <h2>3. Technical Approach</h2>
            <p>Overview of the technical approach.</p>
            <ul>
                <li><a href="#hardware-profiler">Hardware Profiler</a></li>
                <li><a href="#scenarios">Scenarios</a></li>
                <li><a href="#models">Models</a></li>
            </ul>

            <h3 id="hardware-profiler">3.1 Hardware Profiler</h3>
            <p>The Hardware Profiler is a core component of HoliBench that monitors system resources and energy
                consumption in real-time during benchmark execution. It provides granular insights into how models
                utilize the underlying hardware.</p>

            <h4>3.1.1 Architectural Design & Workflow</h4>
            <p>The profiling system utilizes a robust architecture designed for extensibility, managed primarily by two
                core components:</p>
            <ul>
                <li><strong>Profiler Manager</strong> (<code>profiler_manager.py</code>):
                    This class acts as the central orchestrator. It is responsible for <strong>Platform
                        Detection</strong> (automatically identifying if the system is Windows, Linux, Jetson, etc.) and
                    initializing the appropriate profiler instances. It also implements the context manager protocol
                    (<code>__enter__</code>/<code>__exit__</code>), ensuring that all profilers are started and stopped
                    synchronously with the benchmark run.</li>
                <li><strong>Base Device Profiler</strong> (<code>base.py</code>):
                    This abstract base class (ABC) defines the strict contract for all hardware interfaces. It handles
                    the low-level <strong>Concurrency Logic</strong> (managing distinct background threads and
                    stop-events via <code>threading.Event</code>) so that specific implementations (like CPU or GPU
                    profilers) can focus solely on metric collection in their <code>_monitor_process</code> method.</li>
            </ul>
            <p>The runtime workflow consists of four stages:</p>
            <ol>
                <li><strong>Initialization</strong>: The <code>ProfilerManager</code> detects the underlying hardware
                    platform
                    and instantiates the appropriate profiler classes.
                </li>
                <li><strong>Threaded Monitoring</strong>: Each profiler runs in a dedicated background thread (provided
                    by the Base class) to ensure
                    minimal impact on the benchmarking process.</li>
                <li><strong>Sampling Loop</strong>: The profilers wake up at a configured interval (default: 1.0s),
                    collect metrics from system sensors, write raw data to CSV files, and update in-memory
                    statistics.</li>
                <li><strong>Aggregation</strong>: Upon completion, the manager stops the threads, flushes data to disk,
                    and aggregates statistics (min, max, average, totals) for the final report.</li>
            </ol>

            <h4>3.1.2 Outputs</h4>
            <p>For every benchmark run, the profiler generates:</p>
            <ul>
                <li><strong>CSV Time-Series Data</strong>: Detailed, timestamped logs of all metrics (e.g.,
                    <code>cpu_profiler.csv</code>, <code>mac_profiler.csv</code>).
                </li>
                <li><strong>Aggregated Metrics</strong>: Summary statistics included in the final
                    <code>summary.json</code> (e.g., <code>total_energy_joules</code>,
                    <code>average_gpu_utilization</code>).
                </li>
            </ul>



            <h4>3.1.3 Available Profilers</h4>
            <p>HoliBench includes specialized profilers for different hardware architectures, each employing distinct
                strategies to capture high-fidelity metrics:</p>

            <div class="profiler-card">
                <h5>CPU Profiler (Universal) (<code>cpu_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: Uses the cross-platform <code>psutil</code> library to query OS kernel
                    counters for CPU and memory usage.</p>
                <p><strong>Power Monitoring Strategy</strong>:</p>
                <ul>
                    <li><strong>Intel</strong>: Scans <code>/sys/class/powercap/intel-rapl:*</code> directories to read
                        the package-level energy counters (RAPL) in microjoules.</li>
                    <li><strong>AMD</strong>: Inspects <code>/sys/class/hwmon/hwmon*/energy1_input</code> to access the
                        AMD Energy driver metrics.</li>
                    <li><strong>Fallback</strong>: If root access is unavailable or drivers are missing, power metrics
                        are gracefully omitted while still capturing utilization.</li>
                </ul>
            </div>

            <div class="profiler-card">
                <h5>NVIDIA GPU Profiler (<code>nvidia_gpu_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: Interfaces directly with the NVIDIA Management Library (NVML) via
                    <code>pynvml</code> bindings. This avoids the overhead of spawning <code>nvidia-smi</code>
                    subprocesses.
                </p>
                <p><strong>Key Metrics Collected</strong>:</p>
                <ul>
                    <li><strong>Power</strong>: Real-time power draw in milliwatts
                        (<code>nvmlDeviceGetPowerUsage</code>).</li>
                    <li><strong>Utilization</strong>: GPU core and memory controller activity percentages.</li>
                    <li><strong>Memory</strong>: Accurate VRAM usage (used/free/total).</li>
                </ul>
                <p><strong>Multi-GPU</strong>: Automatically enumerates all visible devices and creates independent
                    profiler instances for granular per-card monitoring.</p>
            </div>

            <div class="profiler-card">
                <h5>Mac Profiler (Apple Silicon) (<code>mac_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: Spawns a background <code>subprocess</code> running the native
                    command-line tool <code>powermetrics</code>. This is the only way to access the hardware energy
                    counters on M-series chips.</p>
                <p><strong>Implementation Details</strong>:</p>
                <ul>
                    <li><strong>Sampling</strong>: Runs <code>powermetrics --samplers cpu_power,gpu_power</code> at the
                        configured interval.</li>
                    <li><strong>Parsing</strong>: Uses regex to specific fields from the stdout stream, separately
                        tracking CPU, GPU, and Apple Neural Engine (ANE) power.</li>
                    <li><strong>Integration</strong>: Calculates total energy (Joules) by mathematically integrating the
                        instantaneous power readings over the exact time delta between samples.</li>
                </ul>
            </div>

            <div class="profiler-card">
                <h5>Jetson Profiler (<code>jetson_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: leverages the <code>jetson-stats</code> (jtop) library, which provides
                    a high-level API for reading the specialized sensors on Orin, Xavier, and Nano modules.</p>
                <p><strong>Hardware Rails</strong>: specifically monitors the <code>VDD_GPU_SOC</code> rail to capture
                    the combined power consumption of the GPU and System-on-Chip.</p>
                <p><strong>Thermal/Frequency</strong>: Captures real-time clock frequencies (CPU cores/GPU clusters) and
                    thermal zone temperatures to detect throttling events.</p>
            </div>

            <div class="profiler-card">
                <h5>Raspberry Pi Profiler (<code>pi_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: optimized for the intricacies of the Raspberry Pi ecosystem.</p>
                <p><strong>Power (Pi 5)</strong>: Uses <code>vcgencmd</code> and <code>pmic_read_adc</code> to query the
                    onboard Power
                    Management IC (PMIC), calculating watts determining the real-time voltage and current of the
                    <code>VDD_CORE</code> rail.
                </p>
                <p><strong>Thermal</strong>: Reads raw millidegree Celsius values directly from
                    <code>/sys/class/thermal/thermal_zone0/temp</code>.
                </p>
            </div>

            <h3 id="scenarios">3.2 Scenarios</h3>
            <p>[Details about Scenarios...]</p>
            <p>For a comprehensive list of datasets used across all scenarios, see the <a
                    href="supplementary.html#datasets-used">Datasets Used</a> section in the Supplementary Materials.
            </p>

            <h3 id="models">3.3 Models</h3>
            <p>Models are the core entities being benchmarked. The framework uses a flexible, plugin-based architecture
                to support a wide variety of foundation models, ranging from Large Language Models (LLMs) to
                Vision-Language Models (VLMs) and Time-Series Forecasting models.</p>
            <p>For a detailed list of models, see the <a href="supplementary.html#models-used">Models Used</a> section
                in the Supplementary Materials.</p>

            <h4>3.3.1 Architectural Design</h4>
            <p>The model loading system abstracts the complexities of different libraries (Hugging Face Transformers,
                momentfm, chronos, statsmodels) into a unified interface. The core components are:</p>
            <ul>
                <li><strong>BaseModelLoader</strong> (<code>base.py</code>): The abstract base class that defines the
                    contract for all model loaders. It enforces standard methods like <code>load_model</code>,
                    <code>predict</code>, <code>unload_model</code>, and <code>compute_perplexity</code>.
                </li>
                <li><strong>ModelFactory</strong> (<code>model_factory.py</code>): A factory pattern that dynamically
                    simplifies instantiation. It reads the <code>model_category</code> from the configuration (e.g.,
                    <code>LLM</code>, <code>VLM</code>, <code>TIME_SERIES</code>) and returns the appropriate loader
                    class.
                </li>
                <li><strong>Device Utils</strong> (<code>device_utils.py</code>): A shared utility module that handles
                    complex device logic, including automatic MPS (Apple Silicon) compatibility checks, CUDA
                    availability, and int4/int8 quantization configuration.</li>
            </ul>

            <h4>3.3.2 Supported Model Types</h4>

            <div class="model-card">
                <h5>Large Language Models (LLMs)</h5>
                <p><strong>Loader</strong>: <code>HuggingFaceLLMLoader</code></p>
                <p><strong>Description</strong>: Supports any causal language model available on the Hugging Face Hub
                    (e.g., Llama 2/3, Qwen, Mistral, TinyLlama).</p>
                <p><strong>Key Features</strong>:</p>
                <ul>
                    <li>Automatic 4-bit and 8-bit quantization via <code>bitsandbytes</code>.</li>
                    <li>FlashAttention-2 support for optimized inference.</li>
                    <li>Time-To-First-Token (TTFT) measurement using a custom <code>TTFTStreamer</code>.</li>
                </ul>
            </div>

            <div class="model-card">
                <h5>Vision-Language Models (VLMs)</h5>
                <p><strong>Loader</strong>: <code>HuggingFaceVLMLoader</code></p>
                <p><strong>Description</strong>: Models that can process both text and images (e.g., LLaVA, SmolVLM,
                    Qwen-VL).</p>
                <p><strong>Key Features</strong>:</p>
                <ul>
                    <li>Handles complex multimodal tokenization and image preprocessing.</li>
                    <li>Supports multiple input formats (Chat templates, pure text prompts, separate image inputs).</li>
                    <li>Automatic resizing of images to match model encoder requirements (e.g., 336x336 for LLaVA).</li>
                </ul>
            </div>

            <div class="model-card">
                <h5>Time-Series Models</h5>
                <p><strong>Loader</strong>: <code>HuggingFaceTimeSeriesLoader</code></p>
                <p><strong>Description</strong>: A diverse set of forecasting models including Chronos, MOMENT,
                    PatchTST, and classic ARIMA.</p>
                <p><strong>Key Features</strong>:</p>
                <ul>
                    <li><strong>Chronos</strong>: Uses a tokenizer-based approach to treat time series as language.</li>
                    <li><strong>MOMENT</strong>: A foundation model for time series that uses a multi-patch
                        architecture.</li>
                    <li><strong>ARIMA</strong>: Integrated via <code>statsmodels</code> for distinct statistical
                        benchmarking baselines.</li>
                </ul>
            </div>

            <h4>3.3.3 Loading Workflow</h4>
            <p>The framework implements a robust "safe-loading" mechanism to prevent Out-Of-Memory (OOM) errors,
                particularly on edge devices:</p>
            <ol>
                <li><strong>Configuration Parsing</strong>: The system reads the YAML config to determine the model ID
                    and requested parameters (quantization, max tokens, etc.).</li>
                <li><strong>Device Selection</strong>: It checks for available accelerators (CUDA, MPS). For Apple
                    Silicon, it performs a "pre-flight" check to estimate if the model fits within the 1GB tensor limit
                    of the MPS backend.</li>
                <li><strong>Instantiation</strong>: The model is loaded. If 4-bit quantization is requested, the
                    <code>BitsAndBytesConfig</code> is injected.
                </li>
                <li><strong>Placement</strong>: If not already placed by the loader (e.g. via
                    <code>device_map="auto"</code>), the model is explicitly moved to the target device.
                </li>
            </ol>
        </section>

        <section id="evaluation" class="section">
            <h2>4. Evaluation and Results</h2>
            <p>Overview of evaluation and results.</p>
            <p>For detailed data and raw results, please see the <a href="data_appendix.html">Data Appendix</a>.</p>
        </section>

        <section id="discussions" class="section">
            <h2>5. Discussions and Conclusions</h2>
            <p>[Content for Discussions and Conclusions goes here...]</p>
        </section>

        <section id="references" class="section">
            <h2>6. References</h2>
            <p>For a complete list of references including datasets and libraries used in this project, please see the
                <a href="references.html">References</a> page.
            </p>
        </section>

        <footer>
            <p>&copy; 2025 HoliBench Project</p>
        </footer>
    </div>
</body>

</html>