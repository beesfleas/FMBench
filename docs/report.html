<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <!DOCTYPE html>
    <html lang="en">

    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Report - HoliBench</title>
        <link rel="stylesheet" href="style.css">
    </head>

<body>
    <div class="sidebar">
        <h2>HoliBench</h2>
        <nav>
            <ul>
                <li><a href="index.html">Home</a></li>
                <li><a href="slides.html">Slides</a></li>
                <li><a href="report.html" class="active">Report</a></li>
                <li><a href="data_appendix.html">Data Appendix</a></li>
                <li><a href="supplementary.html">Supplementary</a></li>
                <li><a href="tutorial.html">Tutorial</a></li>
                <li><a href="references.html">References</a></li>
            </ul>
        </nav>
    </div>

    <div class="main-content">
        <h1>Project Report</h1>

        <div class="toc">
            <h3>Table of Contents</h3>
            <ol>
                <li><a href="#introduction">Introduction</a></li>
                <li><a href="#technical-approach">Technical Approach</a></li>
                <li><a href="#evaluation">Results and Findings</a></li>
                <li><a href="#difficulties">Challenges and Lessons Learned</a></li>
                <li><a href="#future_work">Future Work</a></li>
                <li><a href="#references">References</a></li>
            </ol>
        </div>

        <hr>

        <section id="introduction" class="section">
            <h2>1. Introduction: Foundation Model Performance Benchmarking</h2>
            <p>HoliBench is a flexible benchmarking framework designed to evaluate foundation models across diverse
                hardware platforms.
                To understand our approach, we must first look at how foundation models are typically benchmarked today.
                Popular models often compete intensely to see who can achieve better scores on specific capability
                metrics.
            </p>

            <p>These capability metrics vary by model type:</p>
            <ul>
                <li><strong>Large Language Models (LLMs)</strong>: Evaluated on <em>Perplexity</em> (how well they
                    predict the next token), <em>Reasoning</em> (logic and problem solving), and tasks like
                    <em>Summarization</em>.
                </li>
                <li><strong>Visual Language Models (VLMs)</strong>: Tested via <em>Visual Reasoning Tests</em>, <em>OCR
                        Tests</em> (reading text in images), and <em>Counting Tests</em>.</li>
                <li><strong>Time Series Models</strong>: Assessed using statistical error metrics like <em>SMAPE</em>
                    (Symmetric Mean Absolute Percentage Error) and <em>MSLE</em> (Mean Squared Logarithmic Error).</li>
            </ul>

            <h3>1.1 Performance Benchmarking Landscapes</h3>
            <p>A variety of well-established benchmarks exist solely to determine how "good" a model is compared to
                others in terms of these performance capabilities. Prominent examples include:</p>
            <ul>
                <li><strong><a href="references.html#ref-helm">HELM (Holistic Evaluation of Language
                            Models)</a></strong> from Liang et al. (Stanford CRFM), 2022: Evaluates models across a wide
                    breadth of scenarios to ensure holistic coverage.</li>
                <li><strong><a href="references.html#ref-bigbench">Big-Bench (Beyond the Imitation Game
                            Benchmark)</a></strong> from Srivastava et al. (Google), 2022: A diverse, collaborative
                    benchmark for measuring capabilities.</li>
                <li><strong><a href="references.html#ref-lmsys">LMSYS Chatbot Arena</a></strong> from Chiang et al. (UC
                    Berkeley), 2024: Uses an Elo rating system based on human pairwise comparisons.</li>
                <li><strong><a href="references.html#ref-hf-leaderboard">Hugging Face Open LLM leaderboard</a></strong>
                    from Beeching et al. (Hugging Face), 2024: Automates evaluation on standard datasets to rank open
                    models.</li>
            </ul>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/onePillar.png" alt="Performance Benchmarks Only"
                    style="height: 250px; width: auto;">
                <figcaption>Figure 1: Current Benchmarking Landscape focuses heavily on Model Capability (Performance).
                </figcaption>
            </figure>

            <h3>1.2 Hardware Level Metrics</h3>
            <p>While model capability is crucial, it is not the only factor that matters. <strong>Hardware level
                    metrics</strong> are equally critical, especially when deploying models on edge devices, IoT
                sensors, or resource-constrained environments. People are actively solving these efficiency issues and
                benchmarking them.</p>

            <p>Examples of this focus include:</p>
            <ul>
                <li><strong><a href="references.html#ref-neuricam">NeuriCam</a></strong>: A system for key-frame video
                    super-resolution on power-constrained IoT cameras.</li>
                <li><strong><a href="references.html#ref-flashattention">FlashAttention</a></strong>: Introduced by Tri
                    Dao et al., this innovation significantly improved latency by optimizing IO-awareness, benefiting
                    both datacenter and edge deployments.</li>
                <li>Research on <strong><a href="references.html#ref-micro-npu">Benchmarking Ultra-Low-Power
                            micro-NPUs</a></strong> demonstrates the need for specialized evaluation on tiny silicon.
                </li>
            </ul>

            <p>Typically, there are two key metrics for models running on IoTs and SoCs:</p>
            <ul>
                <li><strong>Energy</strong>: Small devices are power-constrained (battery life) and subject to carbon
                    emission concerns.</li>
                <li><strong>Latency</strong>: Real-time tasks require fast responses. Edge devices have less compute,
                    making them naturally slower and more sensitive to inefficiency.</li>
            </ul>

            <p>Several works have addressed these hardware-centric metrics. <strong><a
                        href="references.html#ref-pagoda">Pagoda</a></strong> utilizes a roofline model to analyze
                latency vs. energy and latency vs. performance limits. Other notable efficiency benchmarks include
                <strong><a href="references.html#ref-mltiny">MLPerf Tiny</a></strong> and <strong><a
                        href="references.html#ref-elib">ELIB</a></strong>.
            </p>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/twoPillar.png" alt="Energy and Latency Benchmarks Only"
                    style="height: 250px; width: auto;">
                <figcaption>Figure 2: The Efficiency Benchmarking Landscape focuses on Energy and Latency.</figcaption>
            </figure>

            <h3>1.3 HoliBench</h3>

            <h4>1.3.1 Limitations</h4>
            <p><strong>Hardware Level Fragmentation:</strong> Developers currently have to use different, incompatible
                tools for every hardware platform to measure system efficiency.</p>
            <ul>
                <li><strong>NVIDIA GPUs</strong>: Require interfacing with NVML or spawning nvidia-smi subprocesses.
                </li>
                <li><strong>Apple Silicon</strong>: Require powermetrics (CLI tool).</li>
                <li><strong>Jetson Edge Devices</strong>: Require jetson-stats library to read specialized sensors on
                    Orin/Xavier.</li>
                <li><strong>CPUs</strong>: Require psutil for OS kernel counters.</li>
                <li><strong>Raspberry Pi</strong>: Uses vcgencmd and pmic_read_adc to query onboard Power Management
                    ICs.</li>
            </ul>

            <p><strong>Benchmark Gaps:</strong> Current benchmarking ecosystems suffer from critical limitations:</p>
            <ul>
                <li>Most benchmarks focus on server-grade hardware or pure inference speed.</li>
                <li>There is a lack of Edge-focused benchmarks that treat Energy and Latency as first-class citizens
                    alongside service level goals.</li>
                <li>Small devices are constrained by power and thermal limits (carbon emissions/heat), making these
                    metrics critical for IoT/SoCs.</li>
                <li>There is a need for a library allowing anyone to benchmark models easily across these diverse
                    constraints.</li>
            </ul>

            <h4>1.3.2 Novelty</h4>
            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/threePillar.png" alt="HoliBench Three Pillar Approach"
                    style="height: 250px; width: auto;">
                <figcaption>Figure 3: HoliBench bridges the gap by unifying Performance, Energy, and Latency.
                </figcaption>
            </figure>

            <p>HoliBench introduces several novel contributions to address these challenges:</p>
            <ul>
                <li><strong>Unified "Three Pillar" Approach</strong>: We unify three primary metrics — Latency, Energy,
                    and Performance — into one comprehensive report.</li>
                <li><strong>Modular Design</strong>: Allows for easy addition of new models, scenarios, and hardware
                    profilers.</li>
                <li><strong>Standardization and Control</strong>: Provides a consistent interface for benchmarking
                    across disparate hardware.</li>
                <li><strong>Ease of Use</strong>: YAML-based configuration makes running complex benchmarks simple.</li>
                <li><strong>Hardware Profiler Wrapper</strong>: Features a specialized context manager
                    (<code>__enter__</code>/<code>__exit__</code>) to ensure all profilers start and stop synchronously
                    with the model inference.</li>
            </ul>
        </section>

        <!-- Related Work section removed as it is now integrated into Introduction as subsections 1.1 and 1.2 -->

        <section id="technical-approach" class="section">
            <h2>3. Technical Approach</h2>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/architecture.png" alt="HoliBench System Architecture"
                    style="width: 100%; max-width: 800px; height: auto;">
                <figcaption>Figure 4: HoliBench System Architecture.</figcaption>
            </figure>

            <p>Our overall approach is described in Figure 4. The entrypoint for the user is through the
                <code>run</code> function, but they can also utilize the <code>benchmark_suite</code> in order to get
                comprehensive benchmarks, plots, as well as tables generated for them. The other components of the
                diagram are described in more detail in the following sections.
            </p>

            <p>Generally, the <code>run</code> function connects to a <code>runner</code> that coordinates loading the
                model through <code>model_factory</code>, loading the scenario (task) through <code>scenarios</code>,
                and loading the profiler through <code>profiler_manager</code>. This modular structure allows us to
                easily add new models, scenarios, or profilers. The configurations of these are done through YAML files
                and CLI flags through <a href="references.html#ref-hydra">Hydra from Meta</a>.</p>

            <p>Overview of the technical components:</p>
            <ul>
                <li><a href="#hardware-profiler">Hardware Profiler</a></li>
                <li><a href="#scenarios">Scenarios</a></li>
                <li><a href="#models">Models</a></li>
            </ul>

            <h3 id="hardware-profiler">3.1 Hardware Profiler</h3>
            <p>The Hardware Profiler is a core component of HoliBench that monitors system resources and energy
                consumption in real-time during benchmark execution. It provides granular insights into how models
                utilize the underlying hardware.</p>

            <h4>3.1.1 Architectural Design & Workflow</h4>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/hardwareProfiler.png" alt="Hardware Profiler Architecture"
                    style="width: 100%; max-width: 1000px; height: auto;">
                <figcaption>Figure 5: Hardware Profiler Architecture. <a
                        href="assets/diagrams/img/completeProfilerChart.svg" target="_blank">View full diagram</a>.
                </figcaption>
            </figure>

            <p>The profiling system utilizes a robust architecture designed for extensibility. The <code>runner</code>
                communicates exclusively with the <code>profiler_manager</code>, maintaining a clean separation of
                concerns. The <code>profiler_manager</code> logic ensures that <strong>only one class of profiler is
                    instantiated at a time</strong>, depending on which one it deems necessary for the detected
                hardware.</p>

            <p>The system is managed primarily by two core components:</p>
            <ul>
                <li><strong>Profiler Manager</strong> (<code>profiler_manager.py</code>):
                    This class acts as the central orchestrator. It is responsible for <strong>Platform
                        Detection</strong> (automatically identifying if the system is Windows, Linux, Jetson, etc.) and
                    initializing the appropriate profiler instances. It also implements the context manager protocol
                    (<code>__enter__</code>/<code>__exit__</code>), ensuring that all profilers are started and stopped
                    synchronously with the benchmark run.</li>
                <li><strong>Base Device Profiler</strong> (<code>base.py</code>):
                    This abstract base class (ABC) defines the strict contract for all hardware interfaces. It handles
                    the low-level <strong>Concurrency Logic</strong> (managing distinct background threads and
                    stop-events via <code>threading.Event</code>) so that specific implementations (like CPU or GPU
                    profilers) can focus solely on metric collection in their <code>_monitor_process</code> method.</li>
            </ul>
            <p>The runtime workflow consists of four stages:</p>
            <ol>
                <li><strong>Initialization</strong>: The <code>ProfilerManager</code> detects the underlying hardware
                    platform
                    and instantiates the appropriate profiler classes.
                </li>
                <li><strong>Threaded Monitoring</strong>: Each profiler runs in a dedicated background thread (provided
                    by the Base class) to ensure
                    minimal impact on the benchmarking process.</li>
                <li><strong>Sampling Loop</strong>: The profilers wake up at a configured interval (default: 1.0s),
                    collect metrics from system sensors, write raw data to CSV files, and update in-memory
                    statistics.</li>
                <li><strong>Aggregation</strong>: Upon completion, the manager stops the threads, flushes data to disk,
                    and aggregates statistics (min, max, average, totals) for the final report.</li>
            </ol>

            <h4>3.1.2 Outputs</h4>
            <p>For every benchmark run, the profiler generates:</p>
            <ul>
                <li><strong>CSV Time-Series Data</strong>: Detailed, timestamped logs of all metrics (e.g.,
                    <code>cpu_profiler.csv</code>, <code>mac_profiler.csv</code>).
                </li>
                <li><strong>Aggregated Metrics</strong>: Summary statistics included in the final
                    <code>summary.json</code> (e.g., <code>total_energy_joules</code>,
                    <code>average_gpu_utilization</code>).
                </li>
            </ul>



            <h4>3.1.3 Available Profilers</h4>
            <p>HoliBench includes specialized profilers for different hardware architectures, each employing distinct
                strategies to capture high-fidelity metrics:</p>

            <div class="profiler-card">
                <h5>CPU Profiler (Universal) (<code>cpu_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: Uses the cross-platform <code>psutil</code> library to query OS kernel
                    counters for CPU and memory usage.</p>
                <p><strong>Power Monitoring Strategy</strong>:</p>
                <ul>
                    <li><strong>Intel</strong>: Scans <code>/sys/class/powercap/intel-rapl:*</code> directories to read
                        the package-level energy counters (RAPL) in microjoules.</li>
                    <li><strong>AMD</strong>: Inspects <code>/sys/class/hwmon/hwmon*/energy1_input</code> to access the
                        AMD Energy driver metrics.</li>
                    <li><strong>Fallback</strong>: If root access is unavailable or drivers are missing, power metrics
                        are gracefully omitted while still capturing utilization.</li>
                </ul>
            </div>

            <div class="profiler-card">
                <h5>NVIDIA GPU Profiler (<code>nvidia_gpu_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: Interfaces directly with the NVIDIA Management Library (NVML) via
                    <code>pynvml</code> bindings. This avoids the overhead of spawning <code>nvidia-smi</code>
                    subprocesses.
                </p>
                <p><strong>Key Metrics Collected</strong>:</p>
                <ul>
                    <li><strong>Power</strong>: Real-time power draw in milliwatts
                        (<code>nvmlDeviceGetPowerUsage</code>).</li>
                    <li><strong>Utilization</strong>: GPU core and memory controller activity percentages.</li>
                    <li><strong>Memory</strong>: Accurate VRAM usage (used/free/total).</li>
                </ul>
                <p><strong>Multi-GPU</strong>: Automatically enumerates all visible devices and creates independent
                    profiler instances for granular per-card monitoring.</p>
            </div>

            <div class="profiler-card">
                <h5>Mac Profiler (Apple Silicon) (<code>mac_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: Spawns a background <code>subprocess</code> running the native
                    command-line tool <code>powermetrics</code>. This is the only way to access the hardware energy
                    counters on M-series chips.</p>
                <p><strong>Implementation Details</strong>:</p>
                <ul>
                    <li><strong>Sampling</strong>: Runs <code>powermetrics --samplers cpu_power,gpu_power</code> at the
                        configured interval.</li>
                    <li><strong>Parsing</strong>: Uses regex to specific fields from the stdout stream, separately
                        tracking CPU, GPU, and Apple Neural Engine (ANE) power.</li>
                    <li><strong>Integration</strong>: Calculates total energy (Joules) by mathematically integrating the
                        instantaneous power readings over the exact time delta between samples.</li>
                </ul>
            </div>

            <div class="profiler-card">
                <h5>Jetson Profiler (<code>jetson_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: leverages the <code>jetson-stats</code> (jtop) library, which provides
                    a high-level API for reading the specialized sensors on Orin, Xavier, and Nano modules.</p>
                <p><strong>Hardware Rails</strong>: specifically monitors the <code>VDD_GPU_SOC</code> rail to capture
                    the combined power consumption of the GPU and System-on-Chip.</p>
                <p><strong>Thermal/Frequency</strong>: Captures real-time clock frequencies (CPU cores/GPU clusters) and
                    thermal zone temperatures to detect throttling events.</p>
            </div>

            <div class="profiler-card">
                <h5>Raspberry Pi Profiler (<code>pi_profiler.py</code>)</h5>
                <p><strong>Methodology</strong>: optimized for the intricacies of the Raspberry Pi ecosystem.</p>
                <p><strong>Power (Pi 5)</strong>: Uses <code>vcgencmd</code> and <code>pmic_read_adc</code> to query the
                    onboard Power
                    Management IC (PMIC), calculating watts determining the real-time voltage and current of the
                    <code>VDD_CORE</code> rail.
                </p>
                <p><strong>Thermal</strong>: Reads raw millidegree Celsius values directly from
                    <code>/sys/class/thermal/thermal_zone0/temp</code>.
                </p>
            </div>

            <h3 id="scenarios">3.2 Scenarios</h3>
            <p>Scenarios are the tasks that we verify the model performance on. Examples include standard NLP tasks like
                summarization, or vision tasks like OCR (Optical Character Recognition). A full list of available
                scenarios is shown in Figure 10.</p>

            <p>Figure 11 describes the architectural design of a Scenario. The <code>runner.py</code> file interacts
                only with the abstract `Scenario` class, ensuring decoupling. We primarily use the
                <code>DatasetScenario</code> subclass (formerly known as DatasetScenario), which facilitates loading
                datasets and defining evaluation metrics as per the configuration. Concrete implementations, such as
                <code>NERScenario</code>, inherit from `DatasetScenario` to handle specific dataset loading logic and
                metric calculations.
            </p>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/scenarioList.png" alt="List of Scenarios"
                    style="width: 100%; max-width: 600px; height: auto;">
                <figcaption>Figure 10: Major Available Scenarios.</figcaption>
            </figure>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/scenarioDiagram.png" alt="Scenario Class Diagram"
                    style="width: 100%; max-width: 350px; height: auto;">
                <figcaption>Figure 11: Scenario Class Diagram structure.</figcaption>
            </figure>
            <p>For a comprehensive list of datasets used across all scenarios, see the <a
                    href="supplementary.html#datasets-used">Datasets Used</a> section in the Supplementary Materials.
            </p>

            <h3 id="models">3.3 Models</h3>
            <p>Models are the core entities being benchmarked. The framework uses a flexible, plugin-based architecture
                to support a wide variety of foundation models, ranging from Large Language Models (LLMs) to
                Vision-Language Models (VLMs) and Time-Series Forecasting models.</p>
            <p>For a detailed list of models, see the <a href="supplementary.html#models-used">Models Used</a> section
                in the Supplementary Materials.</p>

            <h4>3.3.1 Architectural Design</h4>
            <p>The model loading system abstracts the complexities of different libraries (Hugging Face Transformers,
                momentfm, chronos, statsmodels) into a unified interface. The core components are:</p>
            <ul>
                <li><strong>BaseModelLoader</strong> (<code>base.py</code>): The abstract base class that defines the
                    contract for all model loaders. It enforces standard methods like <code>load_model</code>,
                    <code>predict</code>, <code>unload_model</code>, and <code>compute_perplexity</code>.
                </li>
                <li><strong>ModelFactory</strong> (<code>model_factory.py</code>): A factory pattern that dynamically
                    simplifies instantiation. It reads the <code>model_category</code> from the configuration (e.g.,
                    <code>LLM</code>, <code>VLM</code>, <code>TIME_SERIES</code>) and returns the appropriate loader
                    class.
                </li>
                <li><strong>Device Utils</strong> (<code>device_utils.py</code>): A shared utility module that handles
                    complex device logic, including automatic MPS (Apple Silicon) compatibility checks, CUDA
                    availability, and int4/int8 quantization configuration.</li>
            </ul>

            <h4>3.3.2 Supported Model Types</h4>

            <div class="model-card">
                <h5>Large Language Models (LLMs)</h5>
                <p><strong>Loader</strong>: <code>HuggingFaceLLMLoader</code></p>
                <p><strong>Description</strong>: Supports any causal language model available on the Hugging Face Hub
                    (e.g., Llama 2/3, Qwen, Mistral, TinyLlama).</p>
                <p><strong>Key Features</strong>:</p>
                <ul>
                    <li>Automatic 4-bit and 8-bit quantization via <code>bitsandbytes</code>.</li>
                    <li>FlashAttention-2 support for optimized inference.</li>
                    <li>Time-To-First-Token (TTFT) measurement using a custom <code>TTFTStreamer</code>.</li>
                </ul>
            </div>

            <div class="model-card">
                <h5>Vision-Language Models (VLMs)</h5>
                <p><strong>Loader</strong>: <code>HuggingFaceVLMLoader</code></p>
                <p><strong>Description</strong>: Models that can process both text and images (e.g., LLaVA, SmolVLM,
                    Qwen-VL).</p>
                <p><strong>Key Features</strong>:</p>
                <ul>
                    <li>Handles complex multimodal tokenization and image preprocessing.</li>
                    <li>Supports multiple input formats (Chat templates, pure text prompts, separate image inputs).</li>
                    <li>Automatic resizing of images to match model encoder requirements (e.g., 336x336 for LLaVA).</li>
                </ul>
            </div>

            <div class="model-card">
                <h5>Time-Series Models</h5>
                <p><strong>Loader</strong>: <code>HuggingFaceTimeSeriesLoader</code></p>
                <p><strong>Description</strong>: A diverse set of forecasting models including Chronos, MOMENT,
                    PatchTST, and classic ARIMA.</p>
                <p><strong>Key Features</strong>:</p>
                <ul>
                    <li><strong>Chronos</strong>: Uses a tokenizer-based approach to treat time series as language.</li>
                    <li><strong>MOMENT</strong>: A foundation model for time series that uses a multi-patch
                        architecture.</li>
                    <li><strong>ARIMA</strong>: Integrated via <code>statsmodels</code> for distinct statistical
                        benchmarking baselines.</li>
                </ul>
            </div>

            <h4>3.3.3 Loading Workflow</h4>
            <p>The framework implements a robust "safe-loading" mechanism to prevent Out-Of-Memory (OOM) errors,
                particularly on edge devices:</p>
            <ol>
                <li><strong>Configuration Parsing</strong>: The system reads the YAML config to determine the model ID
                    and requested parameters (quantization, max tokens, etc.).</li>
                <li><strong>Device Selection</strong>: It checks for available accelerators (CUDA, MPS). For Apple
                    Silicon, it performs a "pre-flight" check to estimate if the model fits within the 1GB tensor limit
                    of the MPS backend.</li>
                <li><strong>Instantiation</strong>: The model is loaded. If 4-bit quantization is requested, the
                    <code>BitsAndBytesConfig</code> is injected.
                </li>
                <li><strong>Placement</strong>: If not already placed by the loader (e.g. via
                    <code>device_map="auto"</code>), the model is explicitly moved to the target device.
                </li>
            </ol>

            <h3 id="configurability">3.4 Configurability (Through Hydra)</h3>
            <p>We use <a href="references.html#ref-hydra">Meta Hydra</a> to pass flags for fine-grained control over
                every
                configuration of the benchmark. For a full list of configurable flags, check out the <a
                    href="tutorial.html">Quickstart Guide</a>. For instructions on adding new models or scenarios,
                please see our <a
                    href="https://github.com/beesfleas/ECM202A_2025Fall_Project_13/?tab=readme-ov-file#adding-new-modelsscenarios">GitHub
                    README</a>.</p>

            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/configChart.png" alt="Configuration Chart"
                    style="width: 100%; max-width: 800px; height: auto;">
                <figcaption>Figure 6: Configuration Options via Hydra.</figcaption>
            </figure>

            <p>Using hydra and our .yaml configuration structure has several benefits including:</p>
            <ul>
                <li><strong>Ease of Use</strong>: Instead of having to change actual code, we can simply add CLI flags.
                </li>
                <li><strong>Modular</strong>: Easy to quickly support new scenarios, new models, and even new devices.
                </li>
                <li><strong>Standardization/Control</strong>: We give the user the ability to keep specific conditions
                    constant if they want to (absolute control).</li>
            </ul>

            <h3 id="benchmark-suite">3.5 Benchmark Suite</h3>
            <p>For comprehensive testing, the <code>benchmark_suite.py</code> script allows extensive suites across
                multiple
                model and scenario combinations. More detail on how to run can be found in the <a
                    href="tutorial.html">Tutorial</a>. For more in-depth explanations on features like graph and report
                generation, see our <a
                    href="https://github.com/beesfleas/ECM202A_2025Fall_Project_13?tab=readme-ov-file#benchmark-suite">GitHub
                    README</a>.</p>

            <p>Command: <code>python benchmark_suite.py</code></p>

            <p>After a suite run completes, the script will automatically generate comparative visualizations.</p>

            <div style="display: flex; justify-content: center; gap: 10px; margin: 2rem 0;">
                <figure style="text-align: center; flex: 1; margin: 0;">
                    <img src="assets/diagrams/img/benchmarkConfig.png" alt="Benchmark Configuration"
                        style="width: 100%; height: auto;">
                    <figcaption>Figure 7: Edit BENCHMARK_CONFIG in suite_config.py to define your test matrix.
                    </figcaption>
                </figure>

                <figure style="text-align: center; flex: 1; margin: 0;">
                    <img src="assets/diagrams/img/benchmarkPreSummary.png" alt="Benchmark Pre-Summary Output"
                        style="width: 100%; height: auto;">
                    <figcaption>Figure 8: Terminal output confirming the benchmark run.</figcaption>
                </figure>

                <figure style="text-align: center; flex: 1; margin: 0;">
                    <img src="assets/diagrams/img/benchmarkSummary.png" alt="Benchmark Summary Output"
                        style="width: 100%; height: auto;">
                    <figcaption>Figure 9: Summary output indicating which tests worked.</figcaption>
                </figure>
            </div>

            <p>Note that the outputs are logged as both <code>.json</code> for the full set of results, and the script
                also
                generates tables and figures for us. This can be seen later in the Results section or on our <a
                    href="https://github.com/beesfleas/ECM202A_2025Fall_Project_13/?tab=readme-ov-file#example-graphs">GitHub
                    README</a>.</p>
        </section>

        <section id="evaluation" class="section">
            <h2>4. Results and Findings</h2>
            <p>The full set of results can be found in the <a href="data_appendix.html">Data Appendix</a>. This section
                will go over a couple case studies with interesting results.</p>

            <h3>Case Study 1: NER Task</h3>

            <h4>Scenario description</h4>
            <p>The NER scenario is implemented to evaluate an LLM's ability to accurately extract specific entities from
                a
                given text.</p>
            <p>NER is a critical capability for Large Language Models (LLMs) for several reasons:</p>
            <ul>
                <li><strong>Information Extraction</strong>: It allows LLMs to structure unstructured data, turning raw
                    text into actionable insights.</li>
                <li><strong>Knowledge Graph Construction</strong>: Identifying entities is the first step in building
                    relationships between them.</li>
                <li><strong>Search and Indexing</strong>: Improves search relevance by understanding the specific
                    entities
                    users are looking for.</li>
                <li><strong>Context Understanding</strong>: Helps the model understand the "who", "what", and "where" of
                    a text passage.</li>
            </ul>

            <p><strong>Class</strong>: The core logic is implemented in the <code>NERScenario</code> class within
                <code>software/components/scenarios/common_nlp_scenarios.py</code>.
            </p>
            <p><strong>Dataset</strong>: By default, it uses the <a href="references.html#ref-wikiann">Wikiann
                    dataset</a>
                (multilingual named entity
                recognition), but this is configurable.</p>
            <p><strong>Prompt Engineering</strong>: The scenario uses a prompt template to instruct the model.</p>
            <ul>
                <li><strong>Default Template</strong>: "Extract named entities from the following text: {input}"</li>
                <li><strong>Input</strong>: A sentence from the dataset.</li>
                <li><strong>Target Output</strong>: A formatted list of entities, e.g., "Barack Obama (PER), USA (LOC)".
                </li>
            </ul>

            <h4>Data</h4>
            <figure style="text-align: center; margin: 2rem 0;">
                <img src="assets/diagrams/img/nerScenarioTable.png" alt="NER Scenario Table (GTX 1070)"
                    style="width: 100%; max-width: 800px; height: auto;">
                <figcaption>Figure 12: NER Task Results (GTX 1070).</figcaption>
            </figure>
            <p>This table reflects the results for a GTX 1070 on the task. The full table is available in the <a
                    href="data_appendix.html">Data Appendix</a>.</p>

            <div style="display: flex; justify-content: center; gap: 10px; margin: 2rem 0;">
                <figure style="text-align: center; flex: 1; margin: 0;">
                    <img src="assets/diagrams/img/ner1070Graph.png" alt="NER Graph GTX 1070"
                        style="width: 100%; height: auto;">
                    <figcaption>Figure 13: NER Results on GTX 1070.</figcaption>
                </figure>
                <figure style="text-align: center; flex: 1; margin: 0;">
                    <img src="assets/diagrams/img/ner3070Graph.png" alt="NER Graph RTX 3070"
                        style="width: 100%; height: auto;">
                    <figcaption>Figure 14: NER Results on RTX 3070.</figcaption>
                </figure>
            </div>

            <p>We have left out the smaller devices for this case study as their small <em>n</em> (number of models) for
                this scenario makes it difficult to draw useful conclusions. From here, we draw our first key
                observation.</p>

            <h4>Observations</h4>
            <p><strong>Diminishing Returns on Accuracy:</strong></p>
            <p>Our analysis reveals a distinct Pareto-optimal frontier where larger models generally yield higher
                accuracy, but at a disproportionately increasing energy cost.</p>
            <ul>
                <li><strong>GTX 1070 Analysis</strong>: Beyond the <strong>0.7 F1-score</strong> threshold, achieving
                    marginal gains requires exponentially more computationally expensive models. The "knee" of the curve
                    suggests that for many applications, <a href="references.html#ref-qwen2.5">Qwen2.5-1.5B</a> offers
                    the best balance.</li>
                <li><strong>RTX 3070 Analysis</strong>: We observe a similar trend. However, three distinct "winners"
                    emerge (highlighted in Figure 14) that optimize for different constraints:
                    <ul>
                        <li><em>Low Power</em>: <a href="references.html#ref-qwen2.5">Qwen2.5-1.5B</a> - Efficient
                            model suitable for strictly constrained environments.</li>
                        <li><em>Balanced</em>: <strong><a href="references.html#ref-qwen3">Qwen 3.0-6B</a></strong> -
                            Offers a significant accuracy jump for
                            moderate energy increase.</li>
                        <li><em>High Performance</em>: <strong><a href="references.html#ref-qwen3">Qwen3-4B</a></strong>
                            - The most accurate model in this set,
                            justifiable only when energy is abundant.</li>
                    </ul>
                </li>
            </ul>
            <p>This highlights the importance of selecting a model not just based on raw capability, but tailored to the
                specific energy budget of the deployment hardware.</p>

            <h3>Case Study 2: Task Summarization Across Devices</h3>

            <h4>Scenario description</h4>
            <p>The Summarization scenario measures an LLM's ability to generate meaningful abstracts for news articles
                or
                other long-form text.</p>
            <p>Summarization is a flagship capability for LLMs because:</p>
            <ul>
                <li><strong>Efficiency</strong>: It helps users quickly digest large volumes of information (news,
                    reports, papers).</li>
                <li><strong>Content Generation</strong>: It demonstrates the model's deep understanding of context,
                    nuance, and hierarchy of importance within a text.</li>
                <li><strong>Cross-domain Utility</strong>: It is applicable in legal, medical, technical, and general
                    domains.</li>
            </ul>

            <p><strong>Class</strong>: The logic is encapsulated in the <code>SummarizationScenario</code> class within
                <code>software/components/scenarios/common_nlp_scenarios.py</code>.
            </p>
            <p><strong>Dataset</strong>: The default configuration uses the <a
                    href="references.html#ref-cnndm">CNN/DailyMail
                    dataset</a> (version 3.0.0).</p>
            <p><strong>Prompt Engineering</strong>:</p>
            <ul>
                <li><strong>Default Template</strong>: "Summarize the following article.\n\nArticle:
                    {input}\n\nSummary:"
                </li>
                <li><strong>Input</strong>: The full text of an article.</li>
                <li><strong>Target Output</strong>: The "highlights" or human-written summary associated with the
                    article.</li>
            </ul>

            <h4>Data</h4>
            <div style="display: grid; grid-template-columns: repeat(2, 1fr); gap: 10px; margin: 2rem 0; width: 100%;">
                <figure style="text-align: center;">
                    <img src="assets/graphs/gtx1070_graphs/summarization_latency_vs_energy.png"
                        alt="GTX 1070 Summarization Latency vs Energy" style="width: 100%; height: auto;">
                    <figcaption>Figure 15: GTX 1070 Latency vs Energy.</figcaption>
                </figure>
                <figure style="text-align: center;">
                    <img src="assets/graphs/rtx3070_graphs/summarization_latency_vs_energy.png"
                        alt="RTX 3070 Summarization Latency vs Energy" style="width: 100%; height: auto;">
                    <figcaption>Figure 16: RTX 3070 Latency vs Energy.</figcaption>
                </figure>
                <figure style="text-align: center;">
                    <img src="assets/graphs/jetson_graphs/summarization_latency_vs_energy.png"
                        alt="Jetson Summarization Latency vs Energy" style="width: 100%; height: auto;">
                    <figcaption>Figure 17: Jetson Latency vs Energy.</figcaption>
                </figure>
                <figure style="text-align: center;">
                    <img src="assets/graphs/pi_graphs/summarization_latency_vs_energy.png"
                        alt="Raspberry Pi Summarization Latency vs Energy" style="width: 100%; height: auto;">
                    <figcaption>Figure 18: Raspberry Pi Latency vs Energy.</figcaption>
                </figure>
            </div>

            <div style="display: flex; justify-content: center; gap: 10px; margin: 2rem 0;">
                <figure style="text-align: center; flex: 1; margin: 0;">
                    <img src="assets/diagrams/img/1070Summarization.png" alt="GTX 1070 Summarization Table"
                        style="width: 100%; height: auto;">
                    <figcaption>Figure 19: GTX 1070 Summarization Results.</figcaption>
                </figure>
                <figure style="text-align: center; flex: 1; margin: 0;">
                    <img src="assets/diagrams/img/3070Summarization.png" alt="RTX 3070 Summarization Table"
                        style="width: 100%; height: auto;">
                    <figcaption>Figure 20: RTX 3070 Summarization Results.</figcaption>
                </figure>
            </div>

            <h4>Observations</h4>
            <p><strong>Linear Trend:</strong></p>
            <p>The first observation is the clear <strong>linear trend</strong> between latency and energy. This is
                evident from the earlier graphs. This relationship makes independent comparisons of latency vs. energy
                less critical and suggests we can use simple linear regression to estimate the energy consumed by any
                arbitrary model given its latency. This significantly simplifies solving for the Pareto optimum, as we
                only need to optimize across 2 parameters.</p>

            <p><strong>Quantization Effects:</strong></p>
            <p>We note interesting findings regarding <strong>quantization</strong> in the tables above:</p>
            <ul>
                <li><strong>Latency Impact</strong>: Contrary to our expectations, quantization did not improve latency,
                    especially on older hardware. It only showed significant improvement on the RTX 3070.
                    <ul>
                        <li><strong>Tensor Cores</strong>: Older devices lack Tensor Cores required for efficient
                            low-precision arithmetic. This hypothesis is supported by the fact that there are
                            significant changes in GPU utilization between quantized and unquantized models on the newer
                            RTX 3070 (which possesses Tensor Cores). For example, the RTX 3070 shows almost double the
                            utilization for the Qwen2-7B model, whereas the older device (GTX 1070) does not show this
                            behavior.</li>
                        <li><strong>Unoptimized Kernels</strong>: Some operations may fall back to slower
                            implementations. For example, we support <a href="references.html#ref-flashattention">Flash
                                Attention</a> via a configuration flag, but since it could only be run on one device
                            accessible to us (RTX 3070), we did not conduct widespread experiments with it.</li>
                        <li><strong>KV Cache</strong>: Quantization allows for a larger KV Cache (memory), although this
                            does not always translate to lower latency.</li>
                    </ul>
                </li>
                <li><strong>Accuracy Stability</strong>: The change in accuracy is minimal between quantized and
                    unquantized models.
                    <ul>
                        <li>Our <em>n</em> for the number of tasks per summarization scenario was low at 20, so it may
                            be difficult to detect small differences between the quantized and unquantized models. It is
                            possible, and quite likely however, that the loss is genuinely not significant for
                            reasonable SLO margins.
                        </li>
                    </ul>
                </li>
            </ul>
        </section>

        <section id="difficulties" class="section">
            <h2>5. Challenges and Lessons Learned</h2>
            <p>Throughout the development of HoliBench, we encountered several technical challenges that shaped our
                design decisions and implementation strategies. The following summarizes the key difficulties and how we
                addressed them.</p>
            <ul>
                <li><strong>Inference Time Constraints</strong>: Model inference, particularly for Vision-Language
                    Models (VLMs), proved to be extremely time-consuming. This significantly extended the duration of
                    comprehensive benchmark runs and limited the number of evaluation samples we could feasibly collect
                    within reasonable timeframes.</li>
                <li><strong>Library Dependency Management</strong>: Different foundation models require distinct and
                    often conflicting library dependencies. To mitigate dependency bloat and potential version
                    conflicts, we implemented a lazy-loading architecture that imports model-specific libraries only
                    when the corresponding model type is requested.</li>
                <li><strong>Jetson JetPack Version Compatibility</strong>: NVIDIA Jetson devices running JetPack 5
                    presented unique challenges. Certain library versions and hardware acceleration features are only
                    available on specific JetPack releases, requiring careful version management and conditional feature
                    enabling. For example, JetPack 5 only supported Python 3.8, which made some libraries incompatible.</li>
                <li><strong>Architecture-Specific Optimizations</strong>: Not all model architectures support the same
                    optimization techniques. For example, FlashAttention is only compatible with certain attention
                    implementations, necessitating runtime detection and graceful fallback mechanisms.</li>
                <li><strong>Maintaining Modularity</strong>: Balancing extensibility with code complexity required
                    continuous refactoring. We prioritized a clean separation of concerns between the profiler,
                    scenario, and model components to ensure that new additions do not introduce regressions or
                    architectural debt.</li>
            </ul>
        </section>

        <section id="future_work" class="section">
            <h2>6. Future Work</h2>
            <p>While HoliBench provides a comprehensive benchmarking framework, several avenues for future development
                remain. The following sections outline planned improvements and new features that will further enhance
                the framework's capabilities.</p>

            <h3>6.1 Planned Improvements</h3>
            <ul>
                <li><strong>Per-Architecture Model Loaders</strong>: Develop specialized model loaders optimized for
                    each hardware architecture (e.g., ARM, x86, RISC-V) to improve model compatibility and maximize
                    hardware performance across diverse deployment targets.</li>
                <li><strong>Expanded Benchmark Coverage</strong>: Extend the benchmark dataset by collecting additional
                    evaluation results across a broader range of models and scenarios. Given the significant time
                    investment required for accuracy and service-level metric evaluations, this remains an ongoing
                    priority.</li>
            </ul>

            <h3>6.2 Latency and Energy Estimator</h3>
            <p>A key future enhancement is the development of a predictive model capable of estimating latency and
                energy consumption for arbitrary PyTorch models without requiring full benchmark execution. The proposed
                approach consists of the following stages:</p>
            <ol>
                <li><strong>Representative Block Formation</strong>: Decompose common model architectures into small,
                    reusable computational blocks (e.g., attention layers, convolutional blocks, feed-forward networks).
                </li>
                <li><strong>Relationship Modeling</strong>: Train a predictive model to learn the relationship between
                    combinations of these representative blocks and their corresponding latency and energy
                    characteristics on target hardware.</li>
                <li><strong>Graph Similarity Analysis</strong>: Develop a graph similarity algorithm that can analyze a
                    novel model's computational graph and map it to combinations of the predefined representative
                    blocks.</li>
                <li><strong>Inference-Time Estimation</strong>: Utilize the trained model to provide rapid latency and
                    energy estimates for new models based on their block decomposition.</li>
            </ol>

            <h3>6.3 Model Selection Decision Tree</h3>
            <p>To assist users in selecting the optimal model for their specific deployment constraints, we plan
                to implement a decision tree system...</p>
            <p><em>[tbd]</em></p>
        </section>

        <section id="references" class="section">
            <h2>7. References</h2>
            <p>For a complete list of references including datasets and libraries used in this project, please see the
                <a href="references.html">References</a> page.
            </p>
        </section>

        <footer>
            <p>&copy; 2025 HoliBench Project</p>
        </footer>
    </div>
</body>

</html>