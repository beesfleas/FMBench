model_id: meta-llama/Llama-3.2-11B-Vision-Instruct
model_category: VLM
max_tokens: 256
quantization: 4
use_flash_attention_2: false
